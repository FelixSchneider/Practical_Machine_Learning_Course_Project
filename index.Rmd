---
title: "Predicting Quality of Execution of Weight Lifting Exercise"
subtitle: "Practical Machine Learning Course Project"
author: "Felix Schneider"
date: "7/31/2020"
output: 
  html_document: 
    toc: yes
    number_sections: yes
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	collapse = TRUE,
	include = TRUE
)
library(dplyr)
library(ggplot2)
library(caret)
library(doParallel)
library(randomForest)
library(e1071)
library(foreach)
library(import)
library(fields) # for 'image.plot()'
```

# Introduction
The purpose of this analysis is to use accelerometer data measured on belt, arm, forearm and dumbbell of 6 individuals performing barbell lifts.
The participants are asked to perform the lifts correctly as well as incorrectly in 4 different manners.
The goal of the analysis is to predict the manner in which they performed the exercise from data that were not used in building the model.

# The Data
## Source
The data come from the following source: http://groupware.les.inf.puc-rio.br/har

> Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

> Read more: http://groupware.les.inf.puc-rio.br/har#wle_paper_section#ixzz6UVzQ1xJp

First, the data is downloaded and loaded into R data frames.
```{r download_and_load_data, cache=TRUE, message=FALSE}
trainURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testURL  <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(trainURL,"pml-training.csv")
download.file(testURL, "pml-testing.csv")
pml_training <- read.csv("pml-training.csv",na.strings=c("NA","#DIV/0!"))
pml_testing  <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!"))
```
The data frames have the following dimensions.
```{r dim_training_testing}
dim(pml_training)
dim(pml_testing)
```
The training data have 19622 observations and 160 variables, and the testing data have 20 observations and 160 variables.

## Missing Values
Check whether there are *any* missing values (`NA`s).
```{r are_there_any_NAs}
any(is.na(pml_training))
any(is.na(pml_testing))
```
In fact, there *are* any missing values (`NA`s) in both the training and testing data sets.

Are there **any** columns that are **all** `NA`s?
```{r any_columns_all_NA}
all_NA <- function(x) all(is.na(x))
pml_training %>% sapply(all_NA) %>% any()
pml_testing %>% sapply(all_NA) %>% any()
```
In fact there *are* **any** columns in `pml_training` and `pml_testing` that are **all** NA.

<!-- Which columns are *all* `NA`s? -->
```{r which_training_columns_are_all_NA, include=FALSE, eval=FALSE}
which(sapply(pml_training, all_NA)) %>% names()
```
<!-- There are a few columns that are *all* `NA`s in the training set. -->
```{r which_testing_columns_are_all_NA, include=FALSE, eval=FALSE}
which(sapply(pml_testing, all_NA)) %>% names()
```
<!-- There are substanitally more columns that are *all* `NA`s in the testing set. -->

How many columns have a certain number of NAs?
```{r how_many_columns_have_how_many_NAs}
table(colSums(is.na(pml_training)))
table(colSums(is.na(pml_testing)))
```
In `pml_testing` all columns are either *no* `NA`s (`60` columns) or *all* `NA`s (`100` columns).
In `pml-training` there are relatively few different values of the number of `NA`s.
There are only 60 columns that have *no* `NA`s. 67 columns have 19216 `NA`s. And 6 columns have 19622 `NA`s, i.e. the number of observations in the data set.

Because we want to use only those variables for training the model that are provided for prediction in the *Course Project Prediction Quiz*, we remove the variables that are `NA` in the pml_**testing** set.
The resulting data frames are called `train_notNA` and `quiz` respectively.
```{r remove_NA_columns}
notNA <- colSums(is.na(pml_testing))==0
train_notNA <- pml_training[,notNA][,-1] # remove first column 'X'
quiz  <- pml_testing[,notNA][,-1] # remove first column 'X'
```
```{r select_notNA_columns, include=FALSE, eval=FALSE}
# try to use 'select()' so select not all NA columns
not_all_NA <- function(x) !all(is.na(x))

notNA2 <- pml_testing %>% # named integer; this is required by 'select()'
            sapply(not_all_NA) %>%
            which()
notNA3 <- pml_testing %>% # named logical; this is the equivalent to the colSums() solution
            sapply(not_all_NA)

train_notNA2 <- pml_training %>%
                  select(as.integer(notNA2),-X) # 'select()' requires an 'integer' or 'character' vector

quiz2 <- pml_testing %>%
          select_if(not_all_NA) %>% # 'select_if()' requires a function that returns 'logical'
          select(-X)
```

# The Features
## Measured Variables
The label `classe` assumes one of 5 distinct values.
```{r unique_train_classe}
unique(train_notNA$classe)
```
The labels correspond to the following manners of executions of the weight lifting exercise.
 
 label | execution
 ------|----------
 A | correct execution
 B | throwing elbows to the front
 C | lifting dumbbell only halfway
 D | lowering dumbbell only halfway
 E | throwing the hips to the front
 
The variables come from 4 sensors (IMUs, Inertial Measurement Unit) mounted on

1. arm
1. forearm
1. belt
1. dumbbell

of the participants.

Each IMU measured the 3 cartesian components (`x`,`y`,`z`) of three different quantities:

1. linear acceleration (`accel`)
1. angular acceleration (`gyros`)
1. earth magnetic field (`magnet`)

Further, the orientation of the IMU was measured with the 3 angles

1. roll
1. pitch
1. yaw

Finally, 8 derived quantities are calculated

1. mean (`avg`)
1. variance (`var`)
1. standard deviation (`stddev`)
1. minimum (`min`)
1. maximum (`max`)
1. amplitude (`amplitude`)
1. kurtosis (`kurtosis`)
1. skewness (`skewness`)

## Selected Features
From the columns without `NA`s, select the measured variables as features.
```{r identify_features}
names(train_notNA)
features <- names(train_notNA)[7:58]
```

```{r select_features}
train <- train_notNA %>%
  select(features,classe) %>%
  mutate(classe=factor(classe))
X_quiz <- quiz %>%
  select(features)
```
The `train` data set is contains the target variable `classe` to be able to use the *formula* interface of the `train` command.
The `X_quiz` data set obviously does not contain a target variable since this is to be predicted.

To visualize the `train` data set, an image is plotted. Before plotting, the data are scaled.
```{r image_plot_of_train}
train[,-53] %>%
  as.matrix() %>% # the 'image.plot()' function requires a 'matrix'
  scale() %>%
  t() %>%
  image.plot(zlim=c(-1,1),axes=FALSE)
```

# Algorithm
## Model
The following questions should be addressed in this report:

- *How is the model built?*
- *How is cross validation used?*
- *What is the expected out-of-sample error?*
- *Why were these choices made?*

The problem is a *multiclass* classification problem.
Since accuracy is a major requirement, and speed is less of a concern, I decide to use a **Random Forrest** model.

I am not sure if the `Accuracy` data that the `train` command generates are *out-of-sample* errors, so I set aside an evaluation data set to evaluate out-of-sample error. The data set that is used to build the model is called `build` and the data set that is used to evaluate the out-of-sample error is called `evalu`.
```{r create_build_and_evalu_partitions}
inBuild <- createDataPartition(y=train$classe, p=0.8, list=FALSE)
build <- train[ inBuild,]
evalu <- train[-inBuild,]
```

<!-- The following chunk is used to reduce the amount of data to achieve moderate calculation times for the 'train' function. Should be `eval=FALSE` in the final commit. -->
```{r create_build_subsample, include=FALSE, eval=FALSE}
subSample <- createDataPartition(y=build$classe, p=1.0, list=FALSE)
build <- build[subSample,]
```

Set resampling method to `"cv"` for cross validation.
The tunig parameter is `mtry` the number of predictors used for fitting the model. No preprocessing is used. I use 5 Cross Validation hold-out samples, and do 2 repeates just to see the effect.
```{r set_traincontrol_parameters}
ctrl <- trainControl(method="cv",
                     number=5, # number of Cross Validation hold-out samples
                     repeats=2, 
                     # preProcOptions=list(thresh=0.95),
                     verboseIter=TRUE,
                     returnResamp="all",
                     savePredictions="final",
                     timingSamps=100)
```

The multicore cluster is initialized and registered for parallel execution.
The current time is recorded in `tic` to measure elapsed time after completion of training.
```{r start_parallel_and_clock}
cl <- makePSOCKcluster(8)
registerDoParallel(cl)
tic <- Sys.time()
```

I chose the train method `parRF` because I want to plot Variable Importance and this method provides this because it requires the package `RandomForest` which creates a model object that accepts the `importance=TRUE` parameter.
To see the performance of the model building process across the tuning parameter, I choose `tuneLength=6` to get Accuracy values for 6 different values of `mtry`, instead of the default which is 3.
```{r train_model, cache=TRUE}
model <- train(classe~., data=build,
               method="parRF",
               # preProcess=c("center","scale","nzv"),
               trControl=ctrl,
               importance=TRUE,
               tuneLength=6)
```
I record the current time `toc`, stop the multicore cluster and print the elapsed time.
```{r stop_parallel_and_clock}
toc <- Sys.time()
stopCluster(cl)
print(toc-tic)
```
As a check, I print the values that the training process recorded for timing. I am not sure how parallel execution is reflected in the times.
```{r print_model_times}
model$times
```
The results of the training process are as follows.
```{r print_model}
model
```
The Accuracy over the different number of predictors is plotted as follows.
```{r plot_model}
ggplot(model)
```
One can see from the numbers and from the plot that the accuracy varies only in a very narrow band across the number of randomly selected predictors.
```{r save_model, include=FALSE, eval=TRUE}
save(model, file="./RData/model.RData")
```

The importance of variables is plotted as follows.
```{r plot_varImp}
ggplot(varImp(model), top=10)
```

## Cross Validation

## Out-Of-Sample Error
To get an estimate for the out-of-sample error, the model is predicted for the hold-out data `evalu`.
With the prediction, a Confusion Matrix is constructed.
```{r confusion_matrix}
X_evalu <- select(evalu,-classe)
confMat <- confusionMatrix(predict(model,X_evalu),evalu$classe)
confMat
```
The proportions in the Confusion Matrix are plotted.
```{r plot_confusion_matrix}
qplot(data=as.data.frame(proportions(confMat$table, margin=1), responseName="Proportion"),
      x=Reference, y=Prediction,
      geom="raster", fill=Proportion)
```

# Prediction
The data records that are to be reported in the Quiz are predicted. The results are hidden since they are evaluated by the Quiz.
```{r predict_quiz, results='hide'}
prediction <- predict(model,X_quiz)
prediction
```

```{r save_prediction, include=FALSE, eval=TRUE}
save(prediction, file="./RData/prediction.RData")
```

# Conclusion