---
title: "Predicting Quality of Execution of Weight Lifting Exercise"
subtitle: "Practical Machine Learning Course Project"
author: "Felix Schneider"
date: "7/31/2020"
output: 
  html_document: 
    toc: yes
    number_sections: yes
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	collapse = TRUE,
	include = TRUE
)
library(dplyr)
library(ggplot2)
library(caret)
library(doParallel)
library(randomForest)
library(e1071)
library(foreach)
library(import)
```

# Introduction
The purpose of this analysis is to use accelerometer data measured on belt, arm, forearm and dumbbell of 6 individuals performing barbell lifts.
The participants are asked to perform the lifts correctly as well as incorrectly in 4 different manners.
The goal of the analysis is to predict the manner in which they performed the exercise from data that were not used in building the model.

# The Data
The data come from this source: http://groupware.les.inf.puc-rio.br/har
```{r download_and_load_data, cache=TRUE, message=FALSE}
trainURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testURL  <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(trainURL,"pml-training.csv")
download.file(testURL, "pml-testing.csv")
pml_training <- read.csv("pml-training.csv",na.strings=c("NA","#DIV/0!"))
pml_testing  <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!"))
```
The data frames have the following dimensions.
```{r dim_training_testing}
dim(pml_training)
dim(pml_testing)
```
The training data have 19622 observations and 160 variables, and the testing data have 20 observations and 160 variables.

Check whether there are any `NA`s.
```{r are_there_any_NAs}
any(is.na(pml_training))
any(is.na(pml_testing))
```
Are there **any** columns that are **all** `NA`s?
```{r any_columns_all_NA}
all_na <- function(x) all(is.na(x))
any(sapply(pml_training, all_na))
any(sapply(pml_testing,  all_na))
```
In fact there are **any** columns in `pml_training` and `pml_testing` that are **all** NA.

Which columns are all `NA`s?
```{r which_training_columns_are_all_NA}
which(sapply(pml_training, all_na)) %>% as.data.frame()
```
```{r which_testing_columns_are_all_NA}
which(sapply(pml_testing, all_na)) %>% as.data.frame()
```
How many columns have a certain number of NAs?
```{r how_many_columns_have_how_many_NAs}
table(colSums(is.na(pml_training)))
table(colSums(is.na(pml_testing)))
```
In `pml_testing` all columns are either *no* `NA`s (`60` columns) or *all* `NA`s (`100` columns).
In `pml-training` there are relatively few different values of the number of `NA`s. There are only 60 columns that have *no* `NA`s. 67 columns have 19216 `NA`s. This is related to the `new_window` variable, as can be see as follows.
```{r check_new_window}
table(pml_training$new_window)
table(pml_testing$new_window)
```
How are the `NA` columns related to the variable `new_window`?
```{r}
new_window_yes <- filter(pml_training,new_window=="yes")
new_window_no  <- filter(pml_training,new_window=="no")
```
```{r}
table(colSums(is.na(new_window_yes)))
table(colSums(is.na(new_window_no)))
```
```{r table_pml-training_new_window}
table(pml_training$new_window)
```
```{r table_pm-training_new_window_is_na, eval=FALSE}
table(pml_training$new_window,is.na(pml_training))
```


Remove the covariates that are `NA` in the pml_**testing** set and call it `quiz` set, because we want to use those covariates for training that are provided for prediction in the Course Project Prediction Quiz. 
```{r remove_NA_columns}
notNA <- colSums(is.na(pml_testing))==0
train_notNA <- pml_training[,notNA][,-1]
quiz  <- pml_testing[,notNA][,-1]
```

# The Features
## Measured Variables
The label `classe` assumes one of 5 distinct values.
```{r unique_train_classe}
unique(train_notNA$classe)
```
The labels correspond to the following executions of the weight lifting exercise.
 
 label | execution
 ------|----------
 A | correct execution
 B | throwing elbows to the front
 C | lifting dumbbell only halfway
 D | lowering dumbbell only halfway
 E | throwing the hips to the front
 
The variables come from 4 sensors (IMUs, Inertial Measurement Unit) mounted on

1. arm
1. forearm
1. belt
1. dumbbell

of the participants.

Each IMU measured the 3 cartesian components (`x`,`y`,`z`) of three different quantities:

1. linear acceleration (`accel`)
1. angular acceleration (`gyros`)
1. earth magnetic field (`magnet`)

Further, the orientation of the IMU was measured with the 3 angles

1. roll
1. pitch
1. yaw

Finally, 8 derived quantities are calculated

1. mean (`avg`)
1. variance (`var`)
1. standard deviation (`stddev`)
1. minimum (`min`)
1. maximum (`max`)
1. amplitude (`amplitude`)
1. kurtosis (`kurtosis`)
1. skewness (`skewness`)

## Features
```{r prepare_train_dataset}
train <- train_notNA %>%
  select(roll_belt:magnet_forearm_z,classe) %>%
  mutate(classe=factor(classe))
X_quiz  <- select(quiz,roll_belt:magnet_forearm_z)
```

# Algorithm
## Model
- *How do I build the model?*
- *How do I use cross validation?*
- *What is the expected out-of-sample error?*
- *Why did I make the choices?*

The problem is a *multiclass* classification problem.
Start with a coarse model to get a feel of the calculation time.

Set aside an evaluation data set to evaluate out-of-sample error.
```{r create_data_partitions}
inBuild <- createDataPartition(y=train$classe, p=0.8, list=FALSE)
build <- train[ inBuild,]
evalu <- train[-inBuild,]
```

```{r create_build_subsample}
subSample <- createDataPartition(y=build$classe, p=0.1, list=FALSE)
build <- build[subSample,]
```

Set resampling method to `"cv"` for cross validation.
```{r set_traincontrol_parameters}
tc <- trainControl(method="cv",
                   preProcOptions=list(thresh=0.95),
                   verboseIter=TRUE,
                   timingSamps=1)
```

I chose the train method `parRF` because I want to plot Variable Importance and this method provides this because it requires the package `RandomForest` which creates a model object that accepts the `importance=TRUE` parameter.
```{r start_parallel_and_clock}
cl <- makePSOCKcluster(8)
registerDoParallel(cl)
tic <- Sys.time()
```

```{r train_model, cache=TRUE}
mdl02parRF <- train(classe~., data=build,
                    method="parRF",
                    preProcess=c("center","scale","nzv"),
                    trControl=tc,
                    importance=TRUE,
                    tuneLength=6)
```

```{r stop_parallel_and_clock}
toc <- Sys.time()
stopCluster(cl)
print(toc-tic)
```

```{r print_mdl_times}
mdl02parRF$times
```

```{r print_model}
mdl02parRF
```

```{r plot_model}
ggplot(mdl02parRF)
```

```{r save_model}
save(mdl02parRF,file="./RData/mdl02RF.RData")
```

```{r plot_varImp}
ggplot(varImp(mdl02parRF),top=10)
```

## Cross Validation

## Out-Of-Sample Error
```{r confusion_matrix}
X_evalu <- select(evalu,-classe)
confMat <- confusionMatrix(predict(mdl02parRF,X_evalu),evalu$classe)
confMat
```

# Prediction
```{r predict, results='hide'}
prd02parRF <- predict(mdl02parRF,X_quiz)
prd02parRF
save(prd02parRF,file="./RData/prd02parRF.RData")
```

# Conclusion